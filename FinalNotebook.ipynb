{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumatra export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook file was written by Andrey Moskaelnko from Purdue University (amoskale@purdue.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This cell will increase the width of the cells to fit more of the screen\n",
    "#adjust the precentage value as you wish\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "import fipy as fp\n",
    "from fipy import numerix as nmx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#record the volume integral of the free energy \n",
    "# equivalent to the average value of the free energy for any cell,\n",
    "# multiplied by the number of cells and the area of each cell\n",
    "# (since this is a 2D domain)        \n",
    "# bulk free energy density\n",
    "def f_0(c):\n",
    "    return rho_s*((c - c_alpha)**2)*((c_beta-c)**2)\n",
    "def f_0_var(c_var):\n",
    "    return 2*rho_s*((c_alpha - c_var)**2 + 4*(c_alpha - c_var)*(c_beta - c_var) + (c_beta - c_var)**2)\n",
    "# free energy\n",
    "def f(c):\n",
    "    return (f_0(c)+ .5*kappa*(c.grad.mag)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block will create a python file in your directory to run the 1a, 1b, 1c, problems from https://pages.nist.gov/chimad-phase-field/hackathon1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile fipy_hackathon_1.py\n",
    "        \n",
    "#read in the parameter file\n",
    "jsonfile = sys.argv[1] \n",
    "if jsonfile:\n",
    "    with open(jsonfile, 'rb') as ff:\n",
    "        params = json.load(ff)\n",
    "        \n",
    "else:\n",
    "    print \"***No parameter file found! ***\"\n",
    "    \n",
    "print 'my params:', params\n",
    "\n",
    "#extract the parameters\n",
    "N = params.get('N', 20)  \n",
    "total_steps = params.get('steps', 2)\n",
    "sumatra_label = params.get('sumatra_label', '')\n",
    "total_sweeps = params.get ('sweeps', 2)\n",
    "duration = params.get('duration', 500)\n",
    "problem = params.get('problem', 'a')\n",
    "tolerance = params.get('tolerance',0.1)\n",
    "solverParam = params.get('solver', 'PCG')\n",
    "\n",
    "\n",
    "c, rho_s, c_alpha, c_beta = sympy.symbols(\"c_var rho_s c_alpha c_beta\")\n",
    "f_0 = rho_s * (c - c_alpha)**2 * (c_beta - c)**2\n",
    "\n",
    "sympy.diff(f_0, c, 2)\n",
    "\n",
    "# command format:\n",
    "\n",
    "\n",
    "if (problem == \"a\"):\n",
    "    print \"Creating mesh for problem a\"\n",
    "    mesh = fp.PeriodicGrid2D(nx=N, ny=N, dx=200.0/float(N), dy=200.0/float(N))\n",
    "    \n",
    "elif (problem == \"b\"):\n",
    "    print \"Creating mesh for problem b\"\n",
    "    mesh = fp.Grid2D(nx=N, ny=N, dx=200.0/float(N), dy=200.0/float(N))\n",
    "    \n",
    "elif (problem == \"c\"):\n",
    "    print \"Creating mesh for problem c\"\n",
    "    Lx=20.\n",
    "    Ly=100.0\n",
    "    dxT=100./N\n",
    "    dyT=100./N\n",
    "    nxT=N\n",
    "    nyT=N/5\n",
    "    mesh = fp.Grid2D(nx=N / 5, ny=N, dx=20./(N/5), dy=100./N) + (fp.Grid2D(nx=N, ny=N / 5, dx=100./N, dy=20./(N/5)) + [[-40],[100]])\n",
    "#    mesh = fp.Grid2D(dx=0.5, dy=0.5, nx=40, ny=200) + (fp.Grid2D(dx=0.5, dy=0.5, nx=200, ny=40) + [[-40],[100]])\n",
    "    mesh = fp.Grid2D(Lx=20., Ly=100.0, nx=N / 5, ny=N) + (fp.Grid2D(Ly=20.0, Lx=100.0, nx=N, ny=N / 5) + [[-40],[100]])\n",
    "       \n",
    "elif (problem == \"d\"):\n",
    "    print \"Creating mesh for problem d\"\n",
    "    r = float(sys.argv[2])\n",
    "    numCellsDesired = int(sys.argv[3])\n",
    "    epsilon = 0.05\n",
    "    cellSize = 16 * np.pi * r**2 / (nmx.sqrt(3.) * numCellsDesired)\n",
    "    cellSize = nmx.sqrt(cellSize)\n",
    "    \n",
    "    substring1 = '''\n",
    "    radius = {0};\n",
    "    cellSize = {1};\n",
    "    '''.format(r, round(cellSize, 6))\n",
    "\n",
    "    mesh = fp.Gmsh2DIn3DSpace(substring1 + '''\n",
    "\n",
    "    // create inner 1/8 shell\n",
    "    Point(1) = {0, 0, 0, cellSize};\n",
    "    Point(2) = {-radius, 0, 0, cellSize};\n",
    "    Point(3) = {0, radius, 0, cellSize};\n",
    "    Point(4) = {0, 0, radius, cellSize};\n",
    "    Circle(1) = {2, 1, 3};\n",
    "    Circle(2) = {4, 1, 2};\n",
    "    Circle(3) = {4, 1, 3};\n",
    "    Line Loop(1) = {1, -3, 2};\n",
    "    Ruled Surface(1) = {1};\n",
    "\n",
    "    // create remaining 7/8 inner shells\n",
    "    t1[] = Rotate {{0,0,1},{0,0,0},Pi/2} {Duplicata{Surface{1};}};\n",
    "    t2[] = Rotate {{0,0,1},{0,0,0},Pi} {Duplicata{Surface{1};}};\n",
    "    t3[] = Rotate {{0,0,1},{0,0,0},Pi*3/2} {Duplicata{Surface{1};}};\n",
    "    t4[] = Rotate {{0,1,0},{0,0,0},-Pi/2} {Duplicata{Surface{1};}};\n",
    "    t5[] = Rotate {{0,0,1},{0,0,0},Pi/2} {Duplicata{Surface{t4[0]};}};\n",
    "    t6[] = Rotate {{0,0,1},{0,0,0},Pi} {Duplicata{Surface{t4[0]};}};\n",
    "    t7[] = Rotate {{0,0,1},{0,0,0},Pi*3/2} {Duplicata{Surface{t4[0]};}};\n",
    "\n",
    "    // create entire inner and outer shell\n",
    "    Surface Loop(100)={1, t1[0],t2[0],t3[0],t7[0],t4[0],t5[0],t6[0]};\n",
    "    ''', order=2.0).extrude(extrudeFunc=lambda r: 1.01*r)\n",
    "\n",
    "c_alpha = 0.3\n",
    "c_beta = 0.7\n",
    "kappa = 2.0\n",
    "M = 5.0\n",
    "c_0 = 0.5\n",
    "epsilon = 0.01\n",
    "rho_s = 5.0\n",
    "\n",
    "\n",
    "\n",
    "# solution variable\n",
    "c_var = fp.CellVariable(mesh=mesh, name=r\"$c$\", hasOld=True)\n",
    "\n",
    "# array of sample c-values: used in f versus c plot\n",
    "vals = np.linspace(-.1, 1.1, 1000)\n",
    "\n",
    "if (problem == 'a' or 'b' or 'c'):\n",
    "    # 2D mesh coordinates\n",
    "    x, y = np.array(mesh.x), np.array(mesh.y)\n",
    "    # initial value for square and T domains\n",
    "    c_var[:] = c_0 + epsilon * (np.cos(0.105 * x) * np.cos(0.11 * y) + (np.cos(0.13 * x) * np.cos(0.087 * y))**2 + np.cos(0.025 * x - 0.15 * y) * np.cos(0.07 * x - 0.02 * y))\n",
    "if (problem == 'd'):\n",
    "    print \"number of cells: \" , mesh.numberOfCells\n",
    "    # 3D mesh coordinates\n",
    "    x, y, z = np.array(mesh.x), np.array(mesh.y), np.array(mesh.z)\n",
    "    \n",
    "    # convert from rectangular to spherical coordinates\n",
    "    theta = fp.CellVariable(name=r\"$\\theta$\", mesh=mesh)\n",
    "    theta = nmx.arctan2(z, nmx.sqrt(x**2 + y**2))\n",
    "    phi = fp.CellVariable(name=r\"$\\phi$\", mesh=mesh)\n",
    "    phi = nmx.arctan2(y, x)\n",
    "     \n",
    "    # initial value for spherical domain\n",
    "    c_var[:]  = c_0 + epsilon * ((np.cos(8*theta))*(np.cos(15*phi)) + ((np.cos(12*theta))*(np.cos(10*phi)))**2 + ((np.cos(2.5*theta - 1.5*phi))*(np.cos(7*theta - 2*phi))))\n",
    "    \n",
    "\n",
    "#Method for making sure we save .mpz.npz at specified dump_times\n",
    "def calc_dt(elapsed_time, dt, dt_old, dump_to_file, dump_times, filename):\n",
    "    if dump_to_file: #if this is true, we have alreay saved the necessary .mpz.npz file\n",
    "        dt = dt_old #reset back to normal dt\n",
    "        dt = dt * 1.1 #continue as normal\n",
    "        dump_to_file = False\n",
    "        # filename = '1a_{0}_step{1}_data_time-{2:.2f}.npz'.format(N, str(steps).rjust(6, '0'), elapsed+dt)\n",
    "    else:\n",
    "        dt_old = dt\n",
    "        dt = dt * 1.1\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        if len(dump_times) > 0:\n",
    "            if (elapsed_time + dt) >= dump_times[0]:\n",
    "                dt = dump_times[0] - elapsed_time\n",
    "                dump_to_file = True\n",
    "                \n",
    "                #dump_time files will have .mpz.npz extension\n",
    "                # filename = '1a_{0}_step{1}_data_time-{2:.2f}.mpz.npz'.format(N, str(steps).rjust(6, '0'), elapsed+dt)\n",
    "                del dump_times[0]\n",
    "\n",
    "    return dt, dt_old, dump_times, dump_to_file, filename\n",
    "    \n",
    "# save elapsed time and free energy at each data point\n",
    "time_data = []\n",
    "cvar_data = []\n",
    "f_data = []\n",
    "# checks whether a folder for the pickles from this simulation exists\n",
    "# if not, creates one in the home directory\n",
    "filepath = os.path.join(os.getcwd(), sumatra_label)\n",
    "\n",
    "\n",
    "# solver equation    \n",
    "eqn = fp.TransientTerm(coeff=1.) == fp.DiffusionTerm(M * f_0_var(c_var)) - fp.DiffusionTerm((M, kappa))\n",
    "\n",
    "elapsed = 0.0\n",
    "steps = 0\n",
    "dt = 0.01\n",
    "dump_times = [1.0, 5.0, 10.0, 20.0, 100.0, 200, 500, 1000, 2000, 3000, 10000] #the specific elapsed times when we want to save out .mpz.npz files\n",
    "dt_old = dt\n",
    "dump_to_file = False\n",
    "filename = 'Andrey Moskalenko'\n",
    "\n",
    "\n",
    "c_var.updateOld()\n",
    "if (solverParam = \"PCG\"):\n",
    "    from fipy.solvers.pysparse import LinearPCGSolver as Solver\n",
    "if (solverParam = \"LU\"):\n",
    "    from fipy.solvers.pysparse import LinearLUSolver as Solver\n",
    "solver = Solver()\n",
    "print \"Starting Solver.\"\n",
    "while (steps <= total_steps) and elapsed <= duration:\n",
    "    res0 = eqn.sweep(c_var, dt=dt, solver=solver)\n",
    "\n",
    "    \n",
    "    for sweeps in range(total_sweeps):\n",
    "        res = eqn.sweep(c_var, dt=dt, solver=solver)\n",
    "                    \n",
    "    if res < res0 * tolerance:  \n",
    "        # anything in this loop will only be executed every 10 steps\n",
    "        if dump_to_file or (steps%10==0):\n",
    "            print steps\n",
    "            print elapsed \n",
    "            print \"Saving data\"\n",
    "            if dump_to_file: filename = '1{3}_{0}_step{1}_data_time-{2:.2f}.mpz.npz'.format(N, str(steps).rjust(6, '0'), elapsed, problem)\n",
    "            else: filename = '1{3}_{0}_step{1}_data_time-{2:.2f}.npz'.format(N, str(steps).rjust(6, '0'), elapsed, problem)\n",
    "            if (problem == 'a') or (problem == 'b'):\n",
    "                np.savez(os.path.join(filepath,filename),\n",
    "                     c_var_array=np.array(c_var),\n",
    "                     dt=dt,\n",
    "                     elapsed=elapsed,\n",
    "                     steps=steps,\n",
    "                     dx=c_var.mesh.dx,\n",
    "                     dy=c_var.mesh.dy,\n",
    "                     nx=c_var.mesh.nx,\n",
    "                     ny=c_var.mesh.ny,\n",
    "                     sweeps = total_sweeps,\n",
    "                     tolerance = tolerance)\n",
    "            if (problem == 'c'):\n",
    "                np.savez(os.path.join(filepath,filename),\n",
    "                     c_var_array=np.array(c_var),\n",
    "                     dt=dt,\n",
    "                     elapsed=elapsed,\n",
    "                     steps=steps,\n",
    "                     dx=dxT,\n",
    "                     dy=dyT,\n",
    "                     nx=nxT,\n",
    "                     ny=nyT,\n",
    "                     sweeps = total_sweeps,\n",
    "                     tolerance = tolerance)\n",
    "\n",
    "                \n",
    "            elif (problem == 'd'):\n",
    "                print \"saving for d!!\"\n",
    "                if dump_to_file: filename = '1d_{0}_step{1}_data_time-{2:.2f}.mpz.npz'.format(N, str(steps).rjust(6, '0'), elapsed)\n",
    "                else: filename = '1d_{0}_step{1}_data_time-{2:.2f}.npz'.format(N, str(steps).rjust(6, '0'), elapsed)\n",
    "                #save_data(elapsed, c_var, )\n",
    "\n",
    "                np.savez(os.path.join(filepath,filename),\n",
    "                     c_var_array=np.array(c_var),\n",
    "                     dt=dt,\n",
    "                     elapsed=elapsed,\n",
    "                     steps=steps,\n",
    "                     dx=c_var.mesh.dx,\n",
    "                     dy=c_var.mesh.dy,\n",
    "                     nx=c_var.mesh.nx,\n",
    "                     ny=c_var.mesh.ny,\n",
    "                     sweeps = total_sweeps,\n",
    "                     tolerance = tolerance)\n",
    "\n",
    "        dt, dt_old, dump_times, dump_to_file, filename = calc_dt(elapsed, dt, dt_old, dump_to_file, dump_times, filename)\n",
    "        steps += 1\n",
    "        elapsed += dt\n",
    "        c_var.updateOld()\n",
    "    else:\n",
    "        dt *= 0.8\n",
    "        c_var[:] = c_var.old\n",
    "\n",
    "# simulation ends\n",
    "print 'steps reached:', steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the json parameter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ParameterFile(N, steps, duration, sweeps, problem, tolerance, solver):\n",
    "    \n",
    "    params = {'N' : N,  'steps' : stp, 'sweeps' : sweeps, 'duration':duration, 'problem':problem, 'tolerance':tolerance, 'solver':solver}\n",
    "\n",
    "    with open('params.json', 'w') as fp:\n",
    "        json.dump(params, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ParameterFile(50, 200, 200, 2, 'a', 0.1, 'PCG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Git Repository "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this demo, I'm assuming that the working directory is a Git repository set up with\n",
    "\n",
    "$ git init\n",
    "\n",
    "gitaddfipytiming.py git ci -m \"Add timing script.\"\n",
    "\n",
    "Sumatra requires that the script is sitting in the a working copy of a repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Sumatra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once the repository is setup, the Sumatra repository can be configured. Here we are using the serial launch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "\\rm -rf .smt #removes any existing sumatra project\n",
    "smt init smt-fipy-demo #chnage this name to whatever you want\n",
    "smt configure --executable=python --main=fipy_hackathon_1.py\n",
    "smt configure --launch_mode=serial\n",
    "smt configure -g uuid\n",
    "smt configure -c store-diff\n",
    "smt configure --addlabel=parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to run Sumatra and store records on the CoRR site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you have to install the proper version of Sumatra to handle the httpStore methods for sotring on CoRR (Contact Faical Yannick Congo at NIST)\n",
    "\n",
    "Create an account on CoRR and find the setting gear at the bottom right, and click on the account button, then copy the 'apt key' that is associated with your profile\n",
    "replace the initializing command with the one here, the others can stay the same\n",
    "\n",
    "NOTE: sumatra's tag function does not work when using CoRR to store the records\n",
    "\n",
    "then run this command to initialize:\n",
    "\n",
    "!smt init -s http://10.200.95.215/api/v0.1/private/\"[USER-IDKEY-PROVIDED-BY-CORR]\" smt-fipy-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation using Sumatra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This command allows you to change the parameters from the params.json file to anything\n",
    "#Sumatra creates a new json file when this happens, which contains the new specified parameters\n",
    "\n",
    "!smt run params.json N=50 steps=200 sweeps=2 duration=200 problem=\"a\" tolerance=0.1 solver=\"PCG\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the simulation is done and Sumatra saves the record, we can export the simulation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "#export the sumatra simulation record\n",
    "!smt export  \n",
    "\n",
    "#save the record into a blank variable called 'data'\n",
    "#in your working Sumatra directory, there will be a .smt/ directory which stores all the records\n",
    "with open('.smt/records_export.json') as ff:\n",
    "    data = json.load(ff)\n",
    "    \n",
    "#save a back up file with a more readable format    \n",
    "with open('record1.json', 'w') as record1:\n",
    "    for entry in range(len(data)):\n",
    "        record1.write(json.dumps(data[entry], sort_keys=True, indent=4, separators=(',', ': ')))\n",
    "        \n",
    "#create a dataframe with which we can now work with\n",
    "df = pandas.DataFrame(data)  #df is now the sumatra dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run simulation in different directories and wish to also include their data with the simulations in your current directory, you can run \"smt export\" in that directory from the command line. Then open the \"directorypath/.smt/records_export.json\" file and save it as df2. Later on you can combine the two dataframe into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df['label'] #to show the Sumatra labels of the simulations in the record\n",
    "\n",
    "df #to see the whole dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractor() : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method finds the simulations which have \".mpz.npz\" files in their Data/ folder and gets all the information out of those files into a dataframe. \n",
    "First you create a dictionary and fill it with all the information from a single simulation, then add the dictionary to the dataframe as a single row.\n",
    "\n",
    "Repeat in a loop for every simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# p = str(df['datastore'][0]['parameters']['root'])\n",
    "# p = p[:-13]\n",
    "# print p\n",
    "# datapath = os.path.join(p, '3ea82cfb13ea')\n",
    "# mfile = glob.glob('{0}/*.mpz.npz'.format(datapath))\n",
    "\n",
    "# # for mpzfile in mfile:\n",
    "# fn = np.load(mfile[0])\n",
    "# for item in fn:\n",
    "#     print item, ' = ', fn[item]\n",
    "        \n",
    "# print mfile\n",
    "dfPCG['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import fipy as fp\n",
    "\n",
    "###FREE ENERGY MATH\n",
    "results = {}\n",
    "c_alpha = 0.3\n",
    "c_beta = 0.7\n",
    "kappa = 2.0\n",
    "# M = 5.0\n",
    "# c_0 = 0.5\n",
    "# epsilon = 0.01\n",
    "rho_s = 5.0\n",
    "\n",
    "#record the volume integral of the free energy \n",
    "# equivalent to the average value of the free energy for any cell,\n",
    "# multiplied by the number of cells and the area of each cell\n",
    "# (since this is a 2D domain)        \n",
    "# bulk free energy density\n",
    "\n",
    "def f_0(c):\n",
    "    return rho_s*((c - c_alpha)**2)*((c_beta-c)**2)\n",
    "# free energy\n",
    "def f(c):\n",
    "    return (f_0(c)+ .5*kappa*(c.grad.mag)**2)\n",
    "\n",
    "#.mpz.npz are marker npz files which are saved at specific elapsed times. That way you can compare the concentration values for the same problem under different grid sizes etc.\n",
    "#and every simulation saves the .mpz.npz files at exactly the same times\n",
    "def extractor(labelz, Lx=200.):  \n",
    "    rows = 0\n",
    "    for label in labelz:\n",
    "        dictt = {} #create a dictionary to fill in with simulation data\n",
    "        dictt.update({'label':lable})\n",
    "        output_filepath = str(df['datastore'][0]['parameters']['root'])\n",
    "        output_filepath = p[:-13] #cuts off the sumatra label from the datapath\n",
    "        datapath = os.path.join(output_filepath, lbl)\n",
    "        mfiles = glob.glob('{0}/*.mpz.npz'.format(datapath)) #get the name of the mpz files with data we need\n",
    "\n",
    "        #create the mesh for simulation associated with this label\n",
    "        #this mesh will only work for problem 1a since it is the periodic mesh\n",
    "        #NEED TO ADD other MESHES for problems b and c\n",
    "        #this code requires an if statement to check for which problem was run in the simulation\n",
    "        #and then create the corresponding mesh for the problem. The free energy calculation changes for the problems as well\n",
    "        fn_0 = np.load(mfile[0]) #first file in the simulaition Data directory\n",
    "        SimMesh = fp.PeriodicGrid2D(nx=fn_0['nx'], ny=fn_0['ny'], dx = Lx / fn_0['nx'], dy = Lx / fn_0['ny'])\n",
    "        cvar_array = fn_0['c_var_array']\n",
    "        c_var = fp.CellVariable(value = cvar_array, mesh = SimMesh)\n",
    "        \n",
    "        #Now we need to read each output file and add all the information from each file as a seprate row into the dataframe\n",
    "        if len(mfiles)>=1:\n",
    "            for mpzfile in mfiles:\n",
    "                fn = np.load(mpzfile)\n",
    "                for item in fn:\n",
    "                    dictt.update({str(item):fn[item]}) #add every variable from the file into the dictionary\n",
    "                \n",
    "                #create blank columns for the dataframe to fill in later\n",
    "                dictt.update({'L1':[]})\n",
    "                dictt.update({'L2':[]})\n",
    "                dictt.update({'Linf':[]})\n",
    "                dictt.update({'real duration':[]})\n",
    "                dictt.update({'memory usage':[]})\n",
    "                \n",
    "                #update the cell variable and append free energy to the dictionary\n",
    "                cvar_array = fn['c_var_array']\n",
    "                c_var[:] = cvar_array\n",
    "                dx = c_var.mesh.dx #Lx / fn['nx']\n",
    "                dy = c_var.mesh.dx #Lx / fn['ny']\n",
    "                free_energy_cellVolAvg = np.mean(f(c_var))*SimMesh.numberOfCells*dx*dy\n",
    "                dictt.update({'free energy at elapsed': free_energy_cellVolAvg }) \n",
    "                \n",
    "                dfC.loc[rows]=dictt        \n",
    "                rows+=1 #row iterator\n",
    "    return dfC #dfC is a compact dataframe with all the information we need\n",
    "\n",
    "#if you wish to combine another dataframe from a separte directory into this dfC dataframe,\n",
    "#you can use the same extractor method, update the output_filepath, and begin the row iterator at the last row of dfC\n",
    "#so rows=len(dfC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe which we need to fill in with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfC = pandas.DataFrame(columns=['label','real duration', 'memory usage', 'steps','elapsed', 'nx', 'ny', 'solver', 'tolerance', 'free energy at elapsed', 'c_var_array', 'dt', 'dx', 'dy', 'sweeps', 'L1', 'L2', 'Linf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the sumatra labels and call the extractor method on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = df['label']\n",
    "extractor(labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting unwanted simulation from the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have unwanted simulations in the dataframe, here is some example code on how to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a temporary dataframe which only includes simulatios with sweeps between 1 and 8\n",
    "dfC_sweepTemp = dfC.query('(sweeps == 1) or (sweeps == 2) or (sweeps == 4) or (sweeps == 3) or (sweeps == 5) or (sweeps == 6) or (sweeps == 7) or (sweeps == 8)')\n",
    "#Then you can also remove a specific simulation by it's sumatra label\n",
    "dfC_sweepTemp2 = dfC_sweepTemp.query('(label != \"509812ef5230\")')\n",
    "\n",
    "#you can also iterate through the rows and change the information in specific cells if needed.\n",
    "for index, row in dfC_sweepTemp2.iterrows():\n",
    "\n",
    "    #now fix the elapsed time for the newest simulation, elapsed is now saved properly so I have to adjust back\n",
    "    if row['label'] == 'a45690313727' or row['label'] == 'ea519a46fe59':\n",
    "        dfC_sweepTemp2.loc[index, 'elapsed'] = dfC_sweepTemp2.loc[index, 'elapsed'] - dfC_sweepTemp2.loc[index, 'dt'] #this is in case the elapsed times are saved wrong in the mpz.npz files for some simulations\n",
    "\n",
    "    #you can find a specific point in a simulation by checking all the conditions at which it might be\n",
    "    if row['label'] == 'ad4019c3836e' and row['steps'] == int(151) and row['sweeps'] == int(8) and row['solver'] == 'PCG':\n",
    "        dfC_sweepTemp2.loc[index, 'elapsed'] = float(200.0) #set the elapsed time for the specific row to 200.0\n",
    "        print 'Elapsed time for ', row['nx'], '-', row['sweeps'], ' is now ', dfC_sweepTemp2.loc[index, 'elapsed'] \n",
    "        \n",
    "dfC_sweepTemp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will show you the first 5 rows, adjust as needed if you wish to see specific rows in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfC_sweepTemp2.loc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c_var Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c_var is the concentration variable value in every cell of the grid for a simulation. We wish to take the finest mesh with the most sweeps, and say that\n",
    "\n",
    "mesh has the most accurate values of concentrations. Call this the Best Simulation. Then we want to compare the other grid sizes to that Best Simulation and \n",
    "\n",
    "take the norms of the concentration values as a figure of merit for comparison of accuracy.\n",
    "\n",
    "The method below will take the datafram which is created by the extractor() above and save the interpolation function into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will create keys for every simulation as identifiers\n",
    "#simulation of grid size 100 with 8 sweeps and PCGSolver will have the key 100-8-PCG\n",
    "#You can also include elapsed time in the key, to separate the various elapsed times in a simulation\n",
    "def Figures_of_Merit(dframe, Lx=200.):\n",
    "    Lx = float(Lx)\n",
    "    N_Best = 0\n",
    "    sweeps_Best = 0\n",
    "    c_var_Best = {}\n",
    "    results = {}\n",
    "    simulations = []\n",
    "    full_keys = []\n",
    "    key_Best = ''\n",
    "    for index, row in dframe.iterrows():\n",
    "        if row['nx'] > N_Best:\n",
    "            N_Best = row['nx']\n",
    "            sweeps_Best = row['sweeps']\n",
    "        elif row['nx'] == N_Best and row['sweeps'] > sweeps_Best:\n",
    "            sweeps_Best = row['sweeps']\n",
    "                    \n",
    "        key_Best = '{0}-{1}'.format(N_Best,sweeps_Best)\n",
    "        print key_Best\n",
    "\n",
    "        key = '{0}-{1}-{2}-{3}'.format(row['nx'], row['sweeps'], row['elapsed'], row['solver'])\n",
    "        print key\n",
    "        sim_key = '{0}-{1}-{2}'.format(row['nx'], row['sweeps'], row['solver'])\n",
    "        if sim_key not in simulations: simulations.append(sim_key)\n",
    "        if key not in full_keys: full_keys.append(key)the\n",
    "        results[key] = {'c_var': np.array(row['c_var_array'])}\n",
    "            \n",
    "    simulations.append(key_Best)\n",
    "    print 'Best simulation has been found! ------ ', key_Best\n",
    "    for key, value in results.iteritems():\n",
    "        mesh_int = fp.Grid2D(nx=N_Best, ny=N_Best, dx=Lx / N_Best, dy=Lx / N_Best)\n",
    "        N_sweeps_elapsed_solver = key.split('-')\n",
    "        N_sweeps_elapsed_solver[0] = int(N_sweeps_elapsed_solver[0])\n",
    "        N_sweeps_elapsed_solver[1] = int(N_sweeps_elapsed_solver[1])\n",
    "        m = fp.Grid2D(nx=N_sweeps_elapsed_solver[0], ny=N_sweeps_elapsed_solver[0], dx=Lx / N_sweeps_elapsed_solver[0], dy=Lx / N_sweeps_elapsed_solver[0])\n",
    "        \n",
    "        v = fp.CellVariable(mesh=m)\n",
    "        v[:] = value['c_var'][:]\n",
    "        v_int = fp.CellVariable(mesh=mesh_int)\n",
    "\n",
    "        v_int[:] = v((mesh_int.x, mesh_int.y), order=1)\n",
    "        \n",
    "        elaps_cvar_Best = '{0}-{1}-{2}-{3}'.format(N_Best,N_sweeps_elapsed_solver[1],  N_sweeps_elapsed_solver[2], 'PCG')\n",
    "        print 'Simulation: ', N_sweeps_elapsed_solver\n",
    "        print elaps_cvar_Best\n",
    "\n",
    "        diff_cvar_Best = np.absolute(results[elaps_cvar_Best]['c_var'] - v_int)\n",
    "#         print 'The diff of {0} with {1}: '.format(key, elaps_cvar_Best), diff_cvar_Best\n",
    "        value['L1'] = np.linalg.norm(diff_cvar_Best,1)\n",
    "        value['L2'] = np.linalg.norm(diff_cvar_Best,2)\n",
    "        value['Linf'] = np.linalg.norm(diff_cvar_Best,np.inf)\n",
    "        \n",
    "    for index, row in dframe.iterrows():\n",
    "            key = '{0}-{1}-{2}-{3}'.format(row['nx'], row['sweeps'], row['elapsed'], row['solver'])\n",
    "            dframe.loc[index, 'L1'] = results[key]['L1']\n",
    "            dframe.loc[index, 'L2'] = results[key]['L2']\n",
    "            dframe.loc[index, 'Linf'] = results[key]['Linf']\n",
    "            \n",
    "    print '-'*100\n",
    "\n",
    "    best_key = '{0}-{1}'.format(N_Best, sweeps_Best)\n",
    "    print '='*100\n",
    "    print dframe['L1']\n",
    "    return best_key, simulations, full_keys, dframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_key, simulations, all_keys, dfC_final = Figures_of_Merit(dfC_sweepTemp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Norms for various grid sizes to compare how sweeps affect Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "cycol = cycle('bgrcmk').next\n",
    "\n",
    "print simulations\n",
    "\n",
    "best_nx, best_sweeps = best_key.split('-')\n",
    "temp_frame = dfC_final.query('(nx == {0}) & (sweeps == {1})'.format(best_nx, best_sweeps))\n",
    "color = cycol()\n",
    "\n",
    "\n",
    "graph_grids = []\n",
    "best_key+=str('-LU')\n",
    "print 'Best simulation key is : ', best_key\n",
    "for key in simulations:\n",
    "    if key != best_key:\n",
    "        print key\n",
    "        nx, sweeps, solvr = key.split('-')\n",
    "\n",
    "        color = cycol()\n",
    "        temp_frame = dfC_final.query('(nx == {0}) & (sweeps == {1})'.format(nx, sweeps))\n",
    "        if nx not in graph_grids:\n",
    "            print 'yay'\n",
    "            graph_grids.append(nx)\n",
    "            if int(nx)==100:\n",
    "                grids_100_L1 = temp_frame.plot('elapsed', 'L1', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 100', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                grids_100_L1.set_xlabel('Norm (log scale)')                \n",
    "\n",
    "                grids_100_L2 = temp_frame.plot('elapsed', 'L2', kind='line', ylim=0, c=color, marker='.',\n",
    "                                               title='L2 for N = 100', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                grids_100_L2.set_xlabel('Norm (log scale)')                \n",
    "\n",
    "                grids_100_Linf = temp_frame.plot('elapsed', 'Linf', kind='line', ylim=0, c=color, marker='.',\n",
    "                                                 title='Linf for N = 100', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                grids_100_Linf.xaxis.set_label('Norm (log scale)')                \n",
    "\n",
    "\n",
    "            if int(nx)==200:\n",
    "#                 print '200 yay'\n",
    "                grids_200_L1 = temp_frame.plot('elapsed', 'L1', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 200', label=key) #, logy=True\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "            \n",
    "                grids_200_L2 = temp_frame.plot('elapsed', 'L2', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L2 for N = 200', label=key) #, logy=True\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                \n",
    "                grids_200_Linf = temp_frame.plot('elapsed', 'Linf', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='Linf for N = 200', label=key) #, logy=True\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                \n",
    "            if int(nx)==400:\n",
    "#                 print '400 yay'\n",
    "                grids_400_L1 = temp_frame.plot('elapsed', 'L1', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 400', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "            \n",
    "                grids_400_L2 = temp_frame.plot('elapsed', 'L2', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L2 for N = 400', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "                \n",
    "                grids_400_Linf = temp_frame.plot('elapsed', 'Linf', kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='Linf for N = 400', label=key)\n",
    "                plt.ylabel('Norm (log scale)')\n",
    "                plt.xlabel('Elapsed Time')\n",
    "        else:\n",
    "            if int(nx)==100:\n",
    "#                 print 'eleeeeeeeeee yay'\n",
    "                temp_frame.plot('elapsed', 'L1', ax = grids_100_L1, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 100', label=key)\n",
    "                temp_frame.plot('elapsed', 'L2', ax = grids_100_L2, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L2 for N = 100', label=key)\n",
    "                temp_frame.plot('elapsed', 'Linf', ax = grids_100_Linf, kind='line', ylim=0, c=color, marker='.', \n",
    "                                                title='Linf for N = 100', label=key)\n",
    "            \n",
    "                axes100 = plt.gca()\n",
    "                # recompute the ax.dataLim\n",
    "                axes100.relim()\n",
    "                # update ax.viewLim using the new dataLim\n",
    "                axes100.autoscale_view()\n",
    "                plt.draw()\n",
    "\n",
    "            if int(nx)==200:\n",
    "                temp_frame.plot('elapsed', 'L1', ax = grids_200_L1, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 200', label=key)\n",
    "                axes200L1 = grids_200_L1.get_axes()\n",
    "                # recompute the ax.dataLim\n",
    "                axes200L1.relim()\n",
    "                # update ax.viewLim using the new dataLim\n",
    "                axes200L1.autoscale_view()\n",
    "                plt.draw()\n",
    "                \n",
    "                \n",
    "                temp_frame.plot('elapsed', 'L2', ax = grids_200_L2, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L2 for N = 200', label=key)\n",
    "                temp_frame.plot('elapsed', 'Linf', ax = grids_200_Linf, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='Linf for N = 200', label=key)\n",
    "            if int(nx)==400:\n",
    "                temp_frame.plot('elapsed', 'L1', ax = grids_400_L1, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L1 for N = 400', label=key)\n",
    "                temp_frame.plot('elapsed', 'L2', ax = grids_400_L2, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='L2 for N = 400', label=key)\n",
    "                temp_frame.plot('elapsed', 'Linf', ax = grids_400_Linf, kind='line', ylim=0, c=color, marker='.', \n",
    "                                               title='Linf for N = 400', label=key)\n",
    "\n",
    "axplt = plt.gca()\n",
    "axplt\n",
    "grids_100_L1.xaxis.set_label('Norm (log scale)')                \n",
    "plt.ylabel('Norm (log scale)')\n",
    "plt.xlabel('Elapsed Time')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add memory and real runtime data and save out the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the memory usage data, you will have to run the mprofile tool in Linux. You can run the fipy_hackathon_1.py file for a short time, maybe 200 steps and it will record the mmeory usage data from it. Then you can get the maximum memory peak as the max CPU usage.\n",
    "\n",
    "Here is the code for how to extract the maximum peaks from the mprofile output files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mprofFiles = glob.glob('*.dat') #get the list of .dat memory profile files\n",
    "memory_dictionary = {'50': None, '100': None, '200': None, '400': None}\n",
    "if len(mprofFiles)>=1:\n",
    "    for memFile in mprofFiles:\n",
    "        ff = open(memFile, 'r')\n",
    "        data = ff.read()\n",
    "        data_list = data.split('\\n')\n",
    "        data_sublist = [line.split(' ') for line in data_list[1:]] #ignores the first entry because it's just text\n",
    "        memdata = [float(item[1]) for item in data_sublist[:-1]] #ignores the last entry, its just a space\n",
    "        memoryPeak = max(memdata) #we have the memory peak for the simulation \n",
    "        print memoryPeak\n",
    "        #now to save it in the proper dictionary entry\n",
    "        gh = memFile.split('_')\n",
    "        size = gh[1].split('N')[1] #split and get the grid size from file name\n",
    "        memory_dictionary[size] = memoryPeak\n",
    "        \n",
    "print memory_dictionary\n",
    "\n",
    "with open('/data/aem1/new1a/memory-profiles/memory_peaks_LU.json', 'w') as fp:\n",
    "    json.dump(memory_dictionary, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the memory usage data and real world duration times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#what are the simulations we are concerned with? better to use the final dataframe in case some simulation were not included from original sumatra dataframe\n",
    "temp_labels = []\n",
    "for thing in np.unique(dfC_final['label']):\n",
    "    print str(thing)\n",
    "    temp_labels.append(str(thing))\n",
    "\n",
    "\n",
    "with open('memory_peaks_LU.json') as ff_LU:\n",
    "    memory_data_LU = json.load(ff_LU)\n",
    "with open('memory_peaks_PCG.json') as ff_PCG:\n",
    "    memory_data_PCG = json.load(ff_PCG)\n",
    "    \n",
    "print memory_data_LU #this is the memory data we will use for all simulations\n",
    "print memory_data_PCG\n",
    "print '-'*100\n",
    "\n",
    "\n",
    "for index, row in dfC_final.iterrows():\n",
    "    if row['solver']==\"LU\":\n",
    "        lbl = str(row['label']) #get the sumatra label\n",
    "        dfC_final.loc[index, 'memory usage'] = memory_data_LU[str(row['nx'])] #input the memory usage data that matches the grid size in the memory data dictionary\n",
    "        dfC_final.loc[index, 'real duration'] = float(df.query('(label==\"{0}\")'.format(lbl))['duration']) #input the real world time data from the original sumatra dataframe\n",
    "    elif row['solver']==\"PCG\":\n",
    "        lbl = str(row['label'])\n",
    "        dfC_final.loc[index, 'memory usage'] = memory_data_PCG[str(row['nx'])]\n",
    "        dfC_final.loc[index, 'real duration'] = float(df.query('(label==\"{0}\")'.format(lbl))['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfC1_dropcol = dfC_final.drop('c_var_array', 1) #drop the array to save space\n",
    "dfC1_dropcol.to_csv('dfC_final_data.csv') #save the file to email to daniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for Time & Free Energy lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will get a list of free energy values and a list of elapsed time values for a specified sumatra label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_0(c):\n",
    "    return rho_s*((c - c_alpha)**2)*((c_beta-c)**2)\n",
    "# free energy\n",
    "def f(c):\n",
    "    return (f_0(c)+ .5*kappa*(c.grad.mag)**2)\n",
    "\n",
    "def EnergyVStimeLists(sumatraLabel):\n",
    "    results = {}\n",
    "    c_alpha = 0.3\n",
    "    c_beta = 0.7\n",
    "    kappa = 2.0\n",
    "    M = 5.0\n",
    "    c_0 = 0.5\n",
    "    epsilon = 0.01\n",
    "    rho_s = 5.0\n",
    "\n",
    "    energyList = []\n",
    "    timeList = []\n",
    "    Lx=200.0\n",
    "    #This code will access every step file saved in the Data/[label] directory \n",
    "    \n",
    "    index = df.label[df.label == str(sumatraLabel)].index.tolist() #this should be a list of a single row index for the specified sumatra label\n",
    "    if len(index) == 1:\n",
    "        output_filepath = df['datastore'][index[0]]['parameters']['root'] \n",
    "\n",
    "    stepfiles = glob.glob('{0}/*.npz'.format(filepath)) #get the list of all step files for simulation\n",
    "\n",
    "    fn_0 = np.load(stepfiles[0])\n",
    "    SimMesh = fp.PeriodicGrid2D(nx=fn['nx'], ny=fn['ny'], dx = Lx / fn['nx'], dy = Lx / fn['ny'])\n",
    "    cvar_array = fn_0['c_var_array']\n",
    "    c_var = fp.CellVariable(value = cvar_array, mesh = SimMesh)\n",
    "\n",
    "    for stpfile in stepfiles:\n",
    "        fn = np.load(stpfile)\n",
    "\n",
    "        cvar_array = fn['c_var_array']\n",
    "        c_var[:] = cvar_array\n",
    "        cells = fn['nx']*fn['ny']\n",
    "        dx = Lx / fn['nx']\n",
    "        dy = Lx / fn['ny']\n",
    "        free_energy_array = f(c_var)\n",
    "        free_energy_cellVolAvg = np.mean(free_energy_array)*cells*dx*dy\n",
    "        energyList.append(free_energy_cellVolAvg)\n",
    "        timeList.append(float(fn['elapsed']))\n",
    "        \n",
    "    return timeList, energyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(timeList, energyList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Energy Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we have to take the elapsed time and cvar from each step file, calculate the energy at that step, then interpolate the array of energies along with time. Save the intepolated function into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate as scpinter\n",
    "\n",
    "results = {}\n",
    "c_alpha = 0.3\n",
    "c_beta = 0.7\n",
    "kappa = 2.0\n",
    "# M = 5.0\n",
    "# c_0 = 0.5\n",
    "# epsilon = 0.01\n",
    "rho_s = 5.0\n",
    "\n",
    "def f_0(c):\n",
    "    return rho_s*((c - c_alpha)**2)*((c_beta-c)**2)\n",
    "def f(c):\n",
    "    return (f_0(c)+ .5*kappa*(c.grad.mag)**2)\n",
    "\n",
    "\n",
    "def freeEnergyInterp(labels, dframe, Lx = 200.):\n",
    "    test_dict = {}\n",
    "    energyFunctions = []\n",
    "    #This code will access every step file saved in the Data/[label] directory \n",
    "    for label in dframe['label']:\n",
    "        e_elaps = []\n",
    "        e_enrg = []\n",
    "        dpath = str(df['datastore'][0]['parameters']['root'])\n",
    "        dpath = dpath[:-13]\n",
    "        filepath = os.path.join(dpath, label)\n",
    "\n",
    "        stepfiles = glob.glob('{0}/*.npz'.format(filepath)) #get the list of all step files for simulation\n",
    "        if len(stepfiles)==0: \n",
    "            energyFunctions.append(None)\n",
    "        else:\n",
    "            fn_0 = np.load(stepfiles[0])\n",
    "            SimMesh = fp.PeriodicGrid2D(nx=fn_0['nx'], ny=fn_0['ny'], dx = Lx / fn_0['nx'], dy = Lx / fn_0['ny'])\n",
    "            cvar_array = fn_0['c_var_array']\n",
    "            c_var = fp.CellVariable(value = cvar_array, mesh = SimMesh)\n",
    "            for stpfile in stepfiles:\n",
    "                fn = np.load(stpfile)\n",
    "\n",
    "                cvar_array = fn['c_var_array']\n",
    "                c_var[:] = cvar_array\n",
    "                cells = fn['nx']*fn['ny']\n",
    "                dx = Lx / fn['nx']\n",
    "                dy = Lx / fn['ny']\n",
    "                free_energy_array = f(c_var)\n",
    "                free_energy_cellVolAvg = np.mean(free_energy_array)*cells*dx*dy\n",
    "\n",
    "\n",
    "                e_elaps.append(fn['elapsed'])\n",
    "                e_enrg.append(free_energy_cellVolAvg)\n",
    "        print 'e_elaps: ', len(e_elaps), '   e_enrg: ', len(e_enrg)\n",
    "        eFunction = scpinter.interp1d(e_elaps, e_enrg, copy=False)\n",
    "        energyFunctions.append(eFunction) #save a list of all the energy functions\n",
    "        print ' '\n",
    "        print '='*100\n",
    "        print 'Number of energy functions: ', len(energyFunctions) \n",
    "        \n",
    "        \n",
    "        #svae values into a dictionary to figure out why norms are 0 for grids 400\n",
    "        if label not in test_dict.keys():\n",
    "            test_dict[label] = {'elapsed':e_elaps, 'energy':e_enrg}\n",
    "            \n",
    "            \n",
    "            \n",
    "    dframe['Free_Energy_Interpolated_Function'] = energyFunctions #add the list as a new column to the dataframe\n",
    "    return test_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now run the method above\n",
    "labels = dfC_final['label']\n",
    "test_dict = freeEnergyInterp(labels, dfC_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Energy norms from the Interpolated functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the free energy function from the best simulation, and do norm calculations with it compared to the other simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This method takes any set of \"ideal times\" and compares the free energy values at those times for every simulation\n",
    "#Create the lists of norms, then insert the lists into the columns of the dataframe\n",
    "\n",
    "ideal_times = []\n",
    "for i in xrange(1, 1000): ideal_times.append(i)\n",
    "#the range of ideal times cannot exceed the maximum time of the shortest simulation. So if you have simulations that ran until 1000, 1500, 3000 elapsed time\n",
    "#you cannot exceed 1000 for the maximum ideal time.\n",
    "    \n",
    "def FreeEnergyCompare(dframe, ideal_times):\n",
    "    L1Norms = [] \n",
    "    L2Norms = []\n",
    "    LinfNorms = []\n",
    "    N_Best = 0\n",
    "    sweeps_Best = 0\n",
    "    \n",
    "    for row in dframe.iterrows():\n",
    "        if row[1]['nx'] >= N_Best and row[1]['sweeps'] >= sweeps_Best:\n",
    "            BestEnergyFunct = row[1]['Free_Energy_Interpolated_Function']\n",
    "            N_Best = row[1]['nx']\n",
    "            sweeps_Best = row[1]['sweeps']\n",
    "    for row in dframe.iterrows():    \n",
    "        efunction = row[1]['Free_Energy_Interpolated_Function']\n",
    "        diff = np.absolute(efunction(ideal_times) - BestEnergyFunct(ideal_times))\n",
    "        L1Norms.append(np.linalg.norm(diff,1)) \n",
    "        L2Norms.append(np.linalg.norm(diff,2)) \n",
    "        LinfNorms.append(np.linalg.norm(diff, np.inf)) \n",
    "    \n",
    "    \n",
    "    dframe['L1 Free Energy Norms'] = L1Norms\n",
    "    dframe['L2 Free Energy Norms'] = L2Norms\n",
    "    dframe['L-infinite Free Energy Norms'] = LinfNorms\n",
    "\n",
    "FreeEnergyCompare(dfC_final, ideal_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory vs CPU time graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot Memory Usage vs CPU for various grid sizes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the color of the dot corresponds to how small the Norm for that grid size is (green = smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe for the memory data to graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_labels = []\n",
    "for thing in np.unique(dfC_final['label']):\n",
    "    print str(thing)\n",
    "    temp_labels.append(str(thing))\n",
    "    \n",
    "print temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#what are the simulations we are concerned with?\n",
    "sim_labels = temp_labels\n",
    "dfMem = pandas.DataFrame(columns=['label','steps','sumatra duration', 'elapsed', 'nx', 'sweeps', 'memory usage', 'L1 Free Energy', 'L2 Free Energy', 'Linf Free Energy'])\n",
    "rows = 0\n",
    "data_dict = {} #the dictionary to which we will add data\n",
    "\n",
    "with open('memory_peaks.json') as ff:\n",
    "    memory_data = json.load(ff)\n",
    "print memory_data #this is the memory data we will use for all simulations\n",
    "    \n",
    "for label in sim_labels:\n",
    "    temp_df = df.query('(label == \"{0}\")'.format(label))\n",
    "    #build the dictionary to add to the memory dataframe\n",
    "    data_dict['sumatra duration'] = float(temp_df['duration'])\n",
    "    data_dict['label'] = label   \n",
    "    data_dict['steps'] = 0\n",
    "    data_dict['elapsed'] = 0\n",
    "    data_dict['nx'] = int(dfC_final.query('(label==\"{0}\")'.format(label)).iloc[0]['nx'])\n",
    "    data_dict['sweeps'] = int(dfC_final.query('(label==\"{0}\")'.format(label)).iloc[0]['sweeps'])\n",
    "    data_dict['memory usage'] = memory_data[str(data_dict['nx'])]\n",
    "    data_dict['L1 Free Energy'] = float(dfC_final.query('(label==\"{0}\")'.format(label)).iloc[0]['L1 Free Energy Norms'])\n",
    "    data_dict['L2 Free Energy'] = float(dfC_final.query('(label==\"{0}\")'.format(label)).iloc[0]['L2 Free Energy Norms'])\n",
    "    data_dict['Linf Free Energy'] = float(dfC_final.query('(label==\"{0}\")'.format(label)).iloc[0]['L-infinite Free Energy Norms'])\n",
    "    \n",
    "    dfMem.loc[rows]=data_dict\n",
    "    rows +=1\n",
    "    \n",
    "dfMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memory_plot = dfMem.plot('sumatra duration', 'memory usage', kind='scatter',ylim=0, title='CPU vs Memory', c=dfMem['Linf Free Energy'], cmap='RdYlGn_r', s=40)\n",
    "\n",
    "fig = plt.figure()\n",
    "axPlot = fig.add_subplot(111, axes = memory_plot.axes)\n",
    "\n",
    "axPlot.plot(dfMem['sumatra duration'], dfMem['memory usage'],'o')\n",
    "plt.ylabel('Peak Memory Usage (MiB)')\n",
    "memory_plot.set_xlabel('Simulation Duration (s)')\n",
    "memory_plot.axes.set_xlabel('Simulation Duration (s)')\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "\n",
    "plt.show()\n",
    "print memory_plot.axes.xaxis.label\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
